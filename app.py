# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KWSKCiQr6eCwSRc4Fg799UnxX1YTojOi

**Saved Model**
"""

import zipfile

zip_path = "/content/saved_bert_emotion_model 1.zip"

if zipfile.is_zipfile(zip_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall("bert_emotion_model")
    print("Extraction complete.")
else:
    print("Not a valid zip file.")

import zipfile

zip_path = "/content/model_folder.zip"

if zipfile.is_zipfile(zip_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall("wav2vec2-emotion-model")
    print("Extraction complete.")
else:
    print("Not a valid zip file.")

import gradio as gr
import torch
import torch.nn.functional as F
import librosa
import numpy as np
from PIL import Image

from transformers import AutoTokenizer, AutoModelForSequenceClassification

pip install gradio


import os
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Wav2Vec2Processor, Wav2Vec2ForSequenceClassification
from PIL import Image
import numpy as np
import librosa
import joblib
from torchvision import models, transforms

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---------- TEXT MODEL ----------
text_model_path = "/content/bert_emotion_model/content/saved_bert_emotion_model"
text_tokenizer = AutoTokenizer.from_pretrained(text_model_path)
text_model = AutoModelForSequenceClassification.from_pretrained(text_model_path).to(device)
label_encoder_text = joblib.load(os.path.join(text_model_path, "label_encoder.joblib"))

def predict_text(text):
    inputs = text_tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        outputs = text_model(**inputs)
        probs = F.softmax(outputs.logits, dim=1)
        pred_idx = torch.argmax(probs, dim=1).item()
        pred_label = label_encoder_text.inverse_transform([pred_idx])[0]
        confidence = round(probs[0][pred_idx].item(), 4)
    return pred_label, confidence


# ---------- VOICE MODEL ----------
voice_model_path = "/content/wav2vec2-emotion-model"
voice_model = Wav2Vec2ForSequenceClassification.from_pretrained(voice_model_path).to(device)
voice_processor = Wav2Vec2Processor.from_pretrained(voice_model_path)
voice_model.eval()
id2label_voice = voice_model.config.id2label

def predict_voice(audio_path):
    speech, sr = librosa.load(audio_path, sr=16000)
    inputs = voice_processor(speech, sampling_rate=16000, return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        logits = voice_model(**inputs).logits
        probs = F.softmax(logits, dim=1)
        pred_idx = torch.argmax(probs, dim=1).item()
        pred_label = id2label_voice[pred_idx]
        confidence = round(probs[0][pred_idx].item(), 4)
    return pred_label, confidence


# ---------- IMAGE MODEL ----------
image_model_path = "/content/final_emotion_image_classifier.pth"
class_labels = ['angry', 'fear', 'happy', 'sad', 'surprise']
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

image_model = models.resnet18(pretrained=False)
image_model.fc = torch.nn.Linear(image_model.fc.in_features, len(class_labels))
image_model.load_state_dict(torch.load(image_model_path, map_location=device))
image_model = image_model.to(device)
image_model.eval()

def predict_image(image: Image.Image):
    img_tensor = transform(image).unsqueeze(0).to(device)
    with torch.no_grad():
        logits = image_model(img_tensor)
        probs = F.softmax(logits, dim=1)
        pred_idx = torch.argmax(probs, dim=1).item()
        pred_label = class_labels[pred_idx]
        confidence = round(probs[0][pred_idx].item(), 4)
    return pred_label, confidence

def fusion_predict(text, audio_path, image):
    text_label, text_conf = predict_text(text)
    voice_label, voice_conf = predict_voice(audio_path)
    image_label, image_conf = predict_image(image)

    labels = [text_label, voice_label, image_label]
    final_label = max(set(labels), key=labels.count)

    final_confidence = (text_conf + voice_conf + image_conf) / 3

    return final_label, final_confidence, text_conf, voice_conf, image_conf

import datetime
import json

def log_interaction(user_id, text, image_path, audio_path, result):
    log = {
        "user_id": user_id,
        "timestamp": str(datetime.datetime.now()),
        "text": text,
        "image": image_path,
        "audio": audio_path,
        "result": result
    }
    with open("user_logs.json", "a") as f:
        f.write(json.dumps(log) + "\n")

def get_mindfulness_suggestions(emotion):
    suggestions = {
        "angry": "Take deep breaths and try to release your frustration.",
        "fear": "Acknowledge your fears, but focus on grounding yourself in the present moment.",
        "happy": "Enjoy the moment and spread your happiness to others.",
        "sad": "Itâ€™s okay to feel sad, try to engage in activities that lift your mood.",
        "surprise": "Take a moment to process the unexpected and stay mindful of your reaction."
    }
    return suggestions.get(emotion, "Try to reflect on your emotions and breathe deeply.")

def get_reflection_prompt(emotion):
    prompts = {
        "angry": "What triggered this anger, and how can you address it calmly?",
        "fear": "What is causing your fear, and how can you overcome it?",
        "happy": "What are you grateful for right now, and how can you spread positivity?",
        "sad": "What steps can you take to move forward and improve your mood?",
        "surprise": "How can you adapt to unexpected changes and stay positive?"
    }
    return prompts.get(emotion, "Reflect on your feelings and how you can improve your emotional well-being.")

import gradio as gr
from PIL import Image

def analyze_emotion(text, image, audio):
    audio_path = "/tmp/temp_audio.wav"
    with open(audio_path, "wb") as f:
        f.write(audio)

    final_label, final_confidence, text_conf, voice_conf, image_conf = fusion_predict(text, audio_path, image)

    mindfulness_suggestion = get_mindfulness_suggestions(final_label)
    reflection_prompt = get_reflection_prompt(final_label)

    confidence_scores = {
        "text_confidence": text_conf,
        "voice_confidence": voice_conf,
        "image_confidence": image_conf
    }

    return final_label, confidence_scores, mindfulness_suggestion, reflection_prompt

gr.Interface(
    fn=analyze_emotion,
    inputs=[
        gr.Textbox(label="Enter journal text"),
        gr.Image(label="Upload an image"),
        gr.Audio(label="Upload your voice")  # Removed source argument
    ],
    outputs=[
        gr.Textbox(label="Detected Emotion"),
        gr.JSON(label="Confidence Scores"),
        gr.Textbox(label="Mindfulness Suggestions"),
        gr.Textbox(label="Reflection Prompt")
    ],
    title="AI-powered Mood Journal & Emotion Tracker"
).launch(share=True)  # Enable sharing to get a public URL